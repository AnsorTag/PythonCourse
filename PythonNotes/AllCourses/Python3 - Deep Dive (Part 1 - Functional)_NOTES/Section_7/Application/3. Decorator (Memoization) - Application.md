#python #application 

---
# PART 1

## Introduction to Memoization using Decorators ğŸ¯

- So far, we've seen simple decorators like:
  - â³ **Timing decorator**
  - ğŸ“‹ **Logging decorator**
- These decorators don't change a function's behavior, but add extra functionality around it.
- However, decorators can **modify** a functionâ€™s behavior too.

---

### Memoization ğŸ§ 
Memoization is a technique used to **cache** the result of a function based on its inputs, preventing redundant calculations for the same inputs.

---

### Fibonacci Sequence Example ğŸ”¢

1. **Fibonacci function using recursion**:
   - Simple but inefficient.
   - Repeated calculations lead to high computational costs.

```python
def fib(n):
    if n < 3:
        return 1
    return fib(n-1) + fib(n-2)
```

Example:  
Calling `fib(10)` will result in **repeated calculations** for values like `fib(7)`, `fib(5)`, etc.

---

## Improving Efficiency with Memoization ğŸï¸

### Approach 1: Using a Class ğŸ“¦

1. **Create a class to handle caching**:
    - Cache is stored in a dictionary where:
        - Key: Input value `n`
        - Value: Corresponding Fibonacci result.

```python
class Fib:
    def __init__(self):
        self.cache = {1: 1, 2: 1}
    
    def fib(self, n):
        if n not in self.cache:
            self.cache[n] = self.fib(n-1) + self.fib(n-2)
        return self.cache[n]
```

- **Result**: The function now **reuses** cached results, improving performance. ğŸ…

---

### Approach 2: Using Closures ğŸ”’

- Closures provide a similar caching mechanism without the need for classes.
- We define an **inner function** within a function that uses an external cache.

```python
def fib():
    cache = {1: 1, 2: 1}
    
    def calc_fib(n):
        if n not in cache:
            cache[n] = calc_fib(n-1) + calc_fib(n-2)
        return cache[n]
    
    return calc_fib
```

- Example: 
   ```python
   f = fib()
   print(f(10))  # 55
   ```
- **Behavior**: Like the class method, this too caches the values effectively.

---

### Approach 3: Using a Decorator ğŸ–Œï¸

We can further improve this by converting it into a **decorator**.

1. **Memoization decorator for the Fibonacci function**:

```python
def memoize_fib(fib):
    cache = {1: 1, 2: 1}
    
    def inner(n):
        if n not in cache:
            cache[n] = fib(n)
        return cache[n]
    
    return inner
```

- **Explanation**:
  - The decorator checks if the result is in the cache.
  - If not, it calls the original `fib` function and stores the result in the cache.

---

### Summary âœï¸

- **Memoization** improves the efficiency of functions, especially recursive ones like Fibonacci.
- You can implement it using **classes**, **closures**, or **decorators**.

---
# PART 2

## ğŸ“œ **Understanding the Decorator**  
In this decorator example, we are not using recursion inside the decorator. The decorator's primary job here is **caching**.

### Key Points:
- The inner function requires **one parameter** (matching what Fibonacci expects).
- We're caching the results for efficient computation.
- ğŸ“ **Takeaway**: The decorator essentially returns a **closure** responsible for caching.

---

## ğŸ” **Caching with a Generic Decorator**  
### Generic Function:
- We initially used `fib` specifically, but if you look at the pattern:
  - Weâ€™re just caching the **function's argument** as the key.
  
### ğŸ—ï¸ Key Concept:  
We can create a **generic memoize function** for any function that takes one parameter (whether it's an integer or string) and stores the result in the cache for future calls.

---

## ğŸ§® **Factorial Example**  
Letâ€™s create a **memoized factorial function** to see how it works.

### Steps:
1. We define a **factorial function** using recursion.
2. Add a print statement to visualize the calculations.
3. Then, apply the **memoize decorator** to improve its efficiency.

### âš™ï¸ How it works:
- It calculates the factorial for numbers like **6** by multiplying the results of previous calculations, i.e., 6 * 5 * 4 * ...  
- Once calculated, if we call the factorial of **6** again, it pulls the result straight from the cache.

---

## ğŸ•‘ **Performance Timing**  
We can use Pythonâ€™s **`perf_counter`** to measure the time taken for these calculations.

```python
from time import perf_counter

start = perf_counter()
fib_result = fib(35)
end = perf_counter()

print(f"Result: {fib_result}, Time taken: {end - start}")
```

### â²ï¸ Performance Gain:
- Without caching, calling **fib(35)** takes a significant amount of time.
- With **memoization**, it returns almost instantly (e.g., **0.0002 seconds**).
  
---

## ğŸ”§ **Improving the Memoize Decorator**  
### Limitations:
1. **Single Parameter Only**: The current decorator only works with functions taking one argument.
2. **Cache Size**: The cache can grow indefinitely, which may cause memory issues. We need to limit it.

### ğŸ§° **Using Python's Built-in LRU Cache**  
Python provides a more powerful decorator: `functools.LRU_cache`.

- **LRU (Least Recently Used)**: This caching mechanism keeps the most recent results and discards the oldest when the cache limit is reached.
- Default cache size: **128** items.

---

### ğŸ›ï¸ **Implementing LRU Cache**  
To use it, simply import and apply it to the function.

```python
from functools import lru_cache

@lru_cache(maxsize=8)  # You can adjust max size here
def fib(n):
    print(f"Calculating fib({n})")
    return n if n < 2 else fib(n-1) + fib(n-2)
```

### How It Works:
- The **first 8 results** are cached.
- Once more than 8 results are computed, the oldest ones are **discarded**.
- Example:
  - Call **fib(8)** â†’ stored in cache.
  - Call **fib(16)** â†’ calculates 9-16 and discards 1-8 from the cache.

---

### ğŸ“‰ **Handling Cache Size**  
- You can set `maxsize` to limit how many items are cached (must be a **power of two** for efficiency).
- Setting `maxsize=None` creates an **unlimited** cache.

---

## ğŸ“ **Summary**
- Memoization with decorators **caches** previous results, greatly improving efficiency for recursive functions like **Fibonacci** and **Factorial**.
- Pythonâ€™s built-in **`lru_cache`** handles cache management for us, making it even more convenient.
- Key trade-off: **Memory vs Speed**. Be mindful of **cache size**.